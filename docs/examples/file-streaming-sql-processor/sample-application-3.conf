# SimpleFileStreamingSqlProcessor Example
#
# Apply the SimpleFileStreamingSqlProcessor to two files, using a more complex input parsing setup and a join query
#
SimpleFileStreamingSqlProcessor: {
  # Input configuration: tables descriptions, variables and the query to be ran
  input: {
    # It contains a map of table names that need to be associated with the files
    # Please keep in mind that for local resources the path needs to be relative to the sql-processor.sh script file
    tables {
      "table1": {
        path: "tmp/in-example-3-1"
        format: "json"
        schema: {
          "type" : "struct",
          "fields" : [ {
            "name" : "id",
            "type" : "string",
            "nullable" : true,
            "metadata" : { }
          }, {
            "name" : "timestamp",
            "type" : "string",
            "nullable" : true,
            "metadata" : { }
          }, {
            "name" : "order",
            "type" : "string",
            "nullable" : true,
            "metadata" : { }
          } ]
        }
      }
      "table2": {
        path: "tmp/in-example-3-2"
        format: "csv"
        schema: {
          "type" : "struct",
          "fields" : [ {
            "name" : "id",
            "type" : "string",
            "nullable" : true,
            "metadata" : { }
          }, {
            "name" : "timestamp",
            "type" : "string",
            "nullable" : true,
            "metadata" : { }
          }, {
            "name" : "address",
            "type" : "string",
            "nullable" : true,
            "metadata" : { }
          } ]
        }
      }
    }
    variables {
      columns: "table1.id, table1.order, table2.address"
    }
    # The query that will be applied on the input tables
    # The query must be written on a single line.
    # All quotes must be escaped.
    sql.line: "SELECT {{columns}} FROM table1 INNER JOIN table2 on table1.id = table2.id "
  }
  output: {
    # The format of the output file; acceptable values are "json", "avro", "json" and "parquet"
    format: "json"
    # The path where the results will be saved
    path: "tmp/out-example-3"
    checkpointLocation: "tmp/out-example-3-cp"
    outputMode: append
    trigger {
      # Once, Continuous or ProcessingTime
      type: ProcessingTime
      interval: 2s
    }
    # The output partition columns
    # partition.columns: ["id"]
  }
}
